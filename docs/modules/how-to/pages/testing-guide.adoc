= AI-Powered Testing Agent Guide
:navtitle: Testing Agent

Understand how the AI-powered testing agent works and how it helps maintain quality.

== Overview

The Content Editorial Assistant uses a *hybrid agent approach* combining:

* *Simple Agent* (GitLab CI) - Reliable test execution
* *AI Agent* (Your configured AI) - Intelligent analysis and insights

== Architecture

[source]
----
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitLab CI (Simple Agent)                â”‚
â”‚  â€¢ Executes all tests daily              â”‚
â”‚  â€¢ Collects results                      â”‚
â”‚  â€¢ Stores data                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Test Runner                             â”‚
â”‚  testing_agent/test_runner.py            â”‚
â”‚  â€¢ Runs 9 test categories                â”‚
â”‚  â€¢ Collects metrics                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Analyzer (Smart Agent)               â”‚
â”‚  testing_agent/ai_test_analyzer.py       â”‚
â”‚  â€¢ Detects patterns                      â”‚
â”‚  â€¢ Identifies root causes                â”‚
â”‚  â€¢ Prioritizes issues                    â”‚
â”‚  â€¢ Generates insights                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Report Generator                        â”‚
â”‚  testing_agent/report_generator.py       â”‚
â”‚  â€¢ Creates HTML/PDF reports              â”‚
â”‚  â€¢ Generates charts                      â”‚
â”‚  â€¢ Lists action items                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
----

== What the AI Agent Does

=== Pattern Detection

The AI analyzes test failures to find common patterns:

[source,json]
----
{
  "test_database_connection": "ConnectionError: pool exhausted",
  "test_concurrent_access": "ConnectionError: pool exhausted",
  "test_transaction_rollback": "ConnectionError: pool exhausted"
}
----

*AI Analysis:*
----
ğŸ§  Pattern Detected: 3 tests failing with identical ConnectionError
Root Cause: Database connection pool exhaustion
Likely Issue: Test fixtures not properly closing connections

Recommended Actions:
1. Add connection cleanup in test teardown (HIGH PRIORITY)
2. Increase pool size in test config (MEDIUM)
3. Add connection leak detection (LOW)

Estimated Fix Time: 2 hours
----

=== Regression Detection

Automatically identifies tests that passed yesterday but fail today:

[source]
----
ğŸ”´ NEW FAILURES (regressions):

1. test_holistic_rewrite
   Error: Confidence score too low (0.65 < 0.80)
   Priority: HIGH
   Estimate: 2 hours
   Action: Check line 245 confidence calculation
----

=== Performance Monitoring

Tracks test execution time over 30 days:

[source]
----
Test Duration: â†“ 5.2% faster âœ…
Average Latency: 1.2s (stable)
Slowest Test: test_large_document (45s)

âš ï¸ Alert: test_api_response_time 20% slower
----

=== Smart Prioritization

AI prioritizes issues based on impact:

[cols="1,2,1"]
|===
|Priority |Issue Type |Action Needed

|*P1 - HIGH*
|Blocking release, new regressions
|Fix today

|*P2 - MEDIUM*
|Performance degradation
|Fix this week

|*P3 - LOW*
|Flaky tests, minor issues
|Fix when convenient
|===

== Daily Report Contents

Every test run generates a comprehensive report:

=== 1. Executive Summary

[source]
----
Status: ğŸŸ¢ HEALTHY
Pass Rate: 96% (382/398 tests)
Duration: 4m 32s
Health Score: A+ (97.5/100)
----

=== 2. Test Results Breakdown

Interactive pie chart showing:

* Passed tests (green)
* Failed tests (red)
* Skipped tests (yellow)

=== 3. Regressions

List of tests that previously passed but now fail.

=== 4. Performance Trends

30-day chart showing:

* Test execution time
* Pass rate trend
* Performance metrics

=== 5. AI-Generated Insights

Intelligent analysis of issues:

[source]
----
ğŸ§  5 ISSUES IDENTIFIED:

[1] Database connection pool exhaustion
    â†’ Add cleanup in test teardown
    
[2] Regex compilation in hot path  
    â†’ Cache compiled patterns
    
[3] Flaky websocket test needs timeout
    â†’ Increase timeout from 5s to 10s
----

=== 6. Prioritized Action Items

Clear, actionable steps:

[source]
----
âœ… WHAT TO DO TODAY:

1. Fix test_holistic_rewrite (HIGH - 2h)
   â†’ Check line 245 confidence calculation
   
2. Optimize SurgicalSnippetProcessor (MEDIUM - 4h)
   â†’ Profile hot paths, cache regex

3. Stabilize websocket test (LOW - 1h)
   â†’ Increase timeout configuration
----

== Test Coverage

=== What's Tested

[cols="2,3"]
|===
|Module |Coverage

|`ambiguity/`
|Ambiguity detection rules

|`app_modules/`
|API routes, websockets, PDF generation

|`database/`
|DAO, models, services

|`error_consolidation/`
|Error merging and prioritization

|`metadata_assistant/`
|Metadata extraction, taxonomy

|`models/`
|Model factory, manager

|`rewriter/`
|Core rewriting engine

|`style_analyzer/`
|Style analysis, readability

|`structural_parsing/`
|Format detection, parsers

|`rules/`
|All 111 editorial rules
|===

=== Feature-Test Mapping

Automatically tracks which features have tests:

[source,bash]
----
# Check coverage status
python -m testing_agent.feature_test_mapper

# Output:
Total Features: 45
Tested: 38 (84%)
Missing Tests: 7

âš ï¸ Untested Features:
- feature_x in module_y
- feature_z in module_w
----

== How Tests Stay In Sync

=== 1. CI Triggers on Code Changes

----
Developer commits â†’ CI runs tests â†’ Reports results
----

=== 2. Feature-Test Mapping

----
New feature added â†’ Mapper detects â†’ Alerts "no tests found"
----

=== 3. Daily Verification

----
Daily run â†’ Checks all features â†’ Reports coverage gaps
----

== AI Configuration

The testing agent uses your existing AI configuration:

[source,bash]
----
# From your .env file
MODEL_PROVIDER=api                    # Your provider (api/ollama/llamastack)
BASE_URL=https://your-api-url.com    # Your API endpoint
MODEL_ID=mistralai/Mistral-7B         # Your model
ACCESS_TOKEN=your-token               # Your credentials
MODEL_TEMPERATURE=0.4                 # Your settings
----

TIP: The testing agent automatically detects and uses your configured AI provider. No separate setup needed!

== Alerts and Notifications

=== Alert Triggers

[cols="2,1,2"]
|===
|Condition |Level |Action

|Pass rate < 85%
|ğŸ”´ CRITICAL
|Immediate fix needed

|New regressions
|ğŸ”´ HIGH
|Review before release

|Performance -20%
|ğŸŸ¡ WARNING
|Investigate today

|Flaky tests > 10%
|ğŸŸ¡ WARNING
|Stabilize this week
|===

=== Configure Notifications

Add to `.env`:

[source,bash]
----
SLACK_WEBHOOK_URL=https://hooks.slack.com/...
EMAIL_RECIPIENTS=team@example.com
----

== Success Metrics

A healthy test suite has:

* âœ… 95%+ pass rate
* âœ… < 5 minutes execution time
* âœ… < 5% flaky tests  
* âœ… 80%+ code coverage
* âœ… Health score: A or A+

Check your current status:

[source,bash]
----
python -m testing_agent.test_runner
----

== Next Steps

* xref:writing-tests.adoc[Learn to write new tests]
* xref:testing-setup.adoc[Setup testing infrastructure]
* xref:../ROOT/pages/index.adoc[Return to documentation home]

== Resources

* link:../../testing_agent/config.py[Testing Configuration Source]
* link:../../testing_agent/ai_test_analyzer.py[AI Analyzer Source]
* link:../../pytest.ini[Pytest Configuration]

