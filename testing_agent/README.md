# ğŸ¤– Content Editorial Assistant - Automated Testing Agent

## Overview

This automated testing agent provides comprehensive, AI-powered testing and reporting for the Content Editorial Assistant application. It runs daily, analyzes test results with AI, and generates actionable reports.

## ğŸ¯ Features

### âœ… **Comprehensive Test Coverage**
- **Unit Tests**: Fast tests for individual components
- **Integration Tests**: Test component interactions
- **API Tests**: Validate all API endpoints
- **E2E Tests**: Complete user workflows
- **UI Tests**: Playwright-based browser testing
- **Performance Tests**: Benchmark critical paths

### ğŸ§  **AI-Powered Analysis**
- Pattern detection in failures
- Root cause analysis
- Regression identification
- Flaky test detection
- Smart prioritization of issues
- Natural language insights

### ğŸ“Š **Comprehensive Reporting**
- Daily HTML/PDF reports
- Interactive charts and visualizations
- Performance trend analysis (30 days)
- Regression detection
- Prioritized action items
- Health score tracking

### ğŸ”„ **Automated Workflows**
- Daily scheduled runs via GitLab CI
- Tests automatically updated with features
- Feature-test mapping
- Coverage tracking

## ğŸ“‹ Quick Start

### 1. Install Dependencies

```bash
# Install test dependencies
pip install -r requirements-test.txt

# Install Playwright browsers (for UI tests)
playwright install chromium
```

### 2. Run Tests Locally

```bash
# Run all tests
python -m testing_agent.test_runner

# Run specific category
pytest -m unit              # Unit tests only
pytest -m integration       # Integration tests
pytest -m ui                # UI tests
pytest -m e2e               # End-to-end tests
pytest -m performance       # Performance tests
```

### 3. View Reports

After running tests, reports are available at:
- HTML: `testing_agent/reports/latest_report.html`
- PDF: `testing_agent/reports/test_report_<timestamp>.pdf`

## ğŸ—ï¸ Architecture

```
testing_agent/
â”œâ”€â”€ __init__.py                    # Module exports
â”œâ”€â”€ test_runner.py                 # Main test orchestrator
â”œâ”€â”€ ai_test_analyzer.py            # AI-powered analysis
â”œâ”€â”€ report_generator.py            # HTML/PDF report generation
â”œâ”€â”€ feature_test_mapper.py         # Feature-test mapping
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ test_history.json          # Historical test data
â”‚   â””â”€â”€ feature_test_mapping.json  # Feature coverage map
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ latest_report.html         # Latest HTML report
â”‚   â””â”€â”€ test_report_*.pdf          # PDF reports
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ latest_results.json        # Latest test results
â”‚   â””â”€â”€ test_results_*.json        # Historical results
â””â”€â”€ coverage/
    â””â”€â”€ unit/, integration/, ...   # Coverage reports by category
```

## ğŸ“Š Report Contents

Each daily report includes:

### 1. **Test Execution Summary**
- Total tests run
- Pass/fail/skip counts
- Pass rate percentage
- Execution duration
- Overall health score

### 2. **Regression Analysis**
- Tests that were passing but now fail
- Comparison with previous run
- Error messages and types

### 3. **Performance Trends**
- 30-day execution time trend
- Slowest 10 tests
- Performance degradation alerts
- P95 latency metrics

### 4. **AI-Generated Insights**
- Root cause analysis
- Pattern detection
- Correlated failures
- Suggested fixes

### 5. **Prioritized Action Items**
- High-priority regressions
- Medium-priority performance issues
- Low-priority flaky tests
- Estimated impact levels

### 6. **Quality Metrics**
- False-positive reduction rate
- User feedback trends
- Confidence score distributions
- ML model performance

### 7. **Test Coverage**
- Features with no tests
- Coverage percentage
- Missing test suggestions

## âš™ï¸ Configuration

### GitLab CI Scheduled Pipeline

Set up daily runs in GitLab:

1. Go to **CI/CD â†’ Schedules**
2. Click **New schedule**
3. Configure:
   - **Description**: Daily Test Suite
   - **Interval pattern**: `0 2 * * *` (2 AM UTC daily)
   - **Target branch**: `main`
   - **Variables**: 
     - `SCHEDULED_JOB` = `daily_tests`
4. Save

### Environment Variables

Create `.env` file for local testing:

```bash
# Application URL for UI tests
APP_URL=http://localhost:5000

# Ollama model for AI analysis
OLLAMA_MODEL=llama3.2:latest

# Playwright configuration
HEADLESS=true
RECORD_VIDEO=false

# Notification settings (optional)
SLACK_WEBHOOK_URL=https://hooks.slack.com/...
EMAIL_RECIPIENTS=team@example.com
```

## ğŸ§ª Writing Tests

### Test Structure

Tests are organized by category with markers:

```python
import pytest

@pytest.mark.unit
def test_example():
    """Unit test example."""
    assert True

@pytest.mark.integration
def test_database_integration():
    """Integration test example."""
    pass

@pytest.mark.ui
def test_user_interface(page):
    """UI test with Playwright."""
    page.goto("http://localhost:5000")
    assert page.title() == "Content Editorial Assistant"

@pytest.mark.e2e
def test_complete_workflow():
    """End-to-end workflow test."""
    pass

@pytest.mark.performance
def test_performance_benchmark(benchmark):
    """Performance test."""
    result = benchmark(my_function)
    assert result < 1.0  # Less than 1 second
```

### Feature-Test Mapping

Ensure tests stay in sync with features:

```python
# When creating a new feature
# 1. Add feature code
# 2. Create corresponding test file

# File: rewriter/new_feature.py
"""New Feature Description"""

# File: tests/test_new_feature.py
import pytest
from rewriter.new_feature import MyFeature

@pytest.mark.unit
def test_new_feature():
    feature = MyFeature()
    assert feature.works()
```

Run mapper to verify coverage:

```bash
python -m testing_agent.feature_test_mapper
```

## ğŸ“ˆ Metrics Tracked

### Test Metrics
- Pass rate (target: 95%+)
- Duration (baseline: 5 minutes)
- Flakiness rate (target: <5%)
- Coverage (target: 80%+)

### Performance Metrics
- Average rewrite latency
- P95 confidence scores
- Database query times
- API response times

### Quality Metrics
- False-positive reduction rate
- User feedback trends
- Regression frequency
- Issue resolution time

## ğŸš¨ Alerts & Notifications

The system generates alerts when:

- **Pass rate drops below 85%** â†’ Critical
- **Performance degrades >20%** â†’ Warning
- **New regressions detected** â†’ High Priority
- **Flaky tests exceed 10%** â†’ Medium Priority

Configure notifications in `testing_agent/test_runner.py`:

```python
def _send_notifications(self, analysis, report_path):
    # Add email notification
    send_email(
        to=os.getenv('EMAIL_RECIPIENTS'),
        subject=f"Test Report: {analysis['summary']['status']}",
        body=generate_email_body(analysis),
        attachments=[report_path]
    )
    
    # Add Slack notification
    post_to_slack(
        webhook_url=os.getenv('SLACK_WEBHOOK_URL'),
        message=generate_slack_message(analysis),
        report_url=report_path
    )
```

## ğŸ” Troubleshooting

### Tests Failing Locally But Pass in CI

```bash
# Ensure same Python version
python --version  # Should match CI (3.11)

# Clear caches
pytest --cache-clear
rm -rf .pytest_cache __pycache__

# Run in same environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-test.txt
```

### UI Tests Timing Out

```bash
# Increase timeout
export PLAYWRIGHT_TIMEOUT=60000  # 60 seconds

# Run with browser visible for debugging
export HEADLESS=false
pytest -m ui -v
```

### AI Analysis Not Working

```bash
# Check Ollama is running
ollama list

# Pull required model
ollama pull llama3.2:latest

# Test connection
curl http://localhost:11434/api/tags
```

## ğŸ“š Best Practices

### 1. **Test Naming**
```python
# Good
def test_analysis_detects_passive_voice():
    ...

# Bad
def test_1():
    ...
```

### 2. **Test Independence**
```python
# Each test should be independent
@pytest.fixture
def clean_database():
    db.clear()
    yield
    db.clear()
```

### 3. **Performance Tests**
```python
# Set reasonable thresholds
@pytest.mark.performance
def test_analysis_performance(benchmark):
    result = benchmark(analyze_text, sample_text)
    assert result.duration < 2.0  # 2 seconds max
```

### 4. **UI Tests**
```python
# Use stable selectors
page.click("[data-testid='analyze-button']")  # Good
page.click(".btn-primary")  # Fragile
```

## ğŸ”„ Continuous Improvement

The testing agent learns over time:

1. **Historical Analysis**: Trends over 90 days
2. **Pattern Detection**: Identifies recurring issues
3. **Smart Prioritization**: Focuses on high-impact items
4. **Coverage Tracking**: Ensures all features tested

## ğŸ“ Support

For issues or questions:

1. Check logs: `testing_agent/results/latest_results.json`
2. View report: `testing_agent/reports/latest_report.html`
3. Review CI pipeline: GitLab CI/CD â†’ Pipelines
4. Contact team: [Your contact info]

## ğŸ‰ Success Criteria

A healthy test suite should have:

- âœ… **95%+ pass rate**
- âœ… **< 5 minutes execution time**
- âœ… **< 5% flaky tests**
- âœ… **80%+ code coverage**
- âœ… **0 regressions**
- âœ… **Health score: A or A+**

## ğŸš€ Future Enhancements

Planned improvements:

- [ ] Visual regression testing
- [ ] Accessibility testing (WCAG compliance)
- [ ] Load testing with Locust
- [ ] Contract testing for APIs
- [ ] Mutation testing
- [ ] Test data generation with AI
- [ ] Automatic test creation for new features
- [ ] Integration with monitoring systems

---

**Last Updated**: 2025-10-01
**Version**: 1.0.0
**Maintained by**: Testing Team

